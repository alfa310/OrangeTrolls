{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Para el procesamiento se utilizara CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ferdinand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "spanish_stopwords = stopwords.words('spanish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '¿',\n",
       " '¡',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Puntuaciones en español\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str,range(10)))\n",
    "non_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer       \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    # remueve las no letras\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    # tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    # stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferdinand\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferdinand\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    0.569285\n",
       "0    0.430715\n",
       "Name: polarity_bin, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_corpus = pd.read_csv(\"TASSTrain.csv\")\n",
    "\n",
    "#Los neutros se eliminan\n",
    "tweets_corpus = tweets_corpus[tweets_corpus.sentimiento != 'NEU']\n",
    "\n",
    "tweets_corpus['polarity_bin'] = 0\n",
    "tweets_corpus.polarity_bin[tweets_corpus.sentimiento.isin(['P', 'P+'])] = 1\n",
    "tweets_corpus.polarity_bin.value_counts(normalize=True)\n",
    "#1 positivo, 0 negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>sentimiento</th>\n",
       "      <th>polarity_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>142391947707940864</td>\n",
       "      <td>@marodriguezb Gracias MAR</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>142416095012339712</td>\n",
       "      <td>Off pensando en el regalito Sinde, la que se v...</td>\n",
       "      <td>N+</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>142422495721562112</td>\n",
       "      <td>Conozco a alguien q es adicto al drama! Ja ja ...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>142483342040907776</td>\n",
       "      <td>Toca @crackoviadeTV3 . Grabación dl especial N...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>142494476051562496</td>\n",
       "      <td>Buen día todos! Lo primero mandar un abrazo gr...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>142496796416016384</td>\n",
       "      <td>Desde el escaño. Todo listo para empezar #endi...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>142497735814287360</td>\n",
       "      <td>Bdías. EM no se ira de puente. Si vosotros os ...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>142499355360903168</td>\n",
       "      <td>Un sistema económico q recorta dinero para pre...</td>\n",
       "      <td>P+</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>142504935853006848</td>\n",
       "      <td>#programascambiados caca d ajuste</td>\n",
       "      <td>N+</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>142507006832553984</td>\n",
       "      <td>Buen viernes</td>\n",
       "      <td>P</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0             tweetid  \\\n",
       "1            2  142391947707940864   \n",
       "2            3  142416095012339712   \n",
       "3            4  142422495721562112   \n",
       "4            6  142483342040907776   \n",
       "5            8  142494476051562496   \n",
       "6            9  142496796416016384   \n",
       "7           10  142497735814287360   \n",
       "8           11  142499355360903168   \n",
       "9           12  142504935853006848   \n",
       "10          13  142507006832553984   \n",
       "\n",
       "                                              content sentimiento  \\\n",
       "1                           @marodriguezb Gracias MAR           P   \n",
       "2   Off pensando en el regalito Sinde, la que se v...          N+   \n",
       "3   Conozco a alguien q es adicto al drama! Ja ja ...          P+   \n",
       "4   Toca @crackoviadeTV3 . Grabación dl especial N...          P+   \n",
       "5   Buen día todos! Lo primero mandar un abrazo gr...          P+   \n",
       "6   Desde el escaño. Todo listo para empezar #endi...          P+   \n",
       "7   Bdías. EM no se ira de puente. Si vosotros os ...          P+   \n",
       "8   Un sistema económico q recorta dinero para pre...          P+   \n",
       "9                   #programascambiados caca d ajuste          N+   \n",
       "10                                       Buen viernes           P   \n",
       "\n",
       "    polarity_bin  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  \n",
       "5              1  \n",
       "6              1  \n",
       "7              1  \n",
       "8              1  \n",
       "9              0  \n",
       "10             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5066"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el vectorizer y creamos un pipeline de vectorizer ; classificador\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('cls', LinearSVC()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el vectorizer de nuevo, se define el clasificador y se entrena\n",
    "\n",
    "model = LinearSVC(C=.2, loss='squared_hinge',max_iter=1000,multi_class='ovr',\n",
    "              random_state=None,\n",
    "              penalty='l2',\n",
    "              tol=0.0001\n",
    ")\n",
    "\n",
    "#parametros\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    stop_words = spanish_stopwords,\n",
    "    min_df = 50,\n",
    "    max_df = 1.9,\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "corpus_data_features = vectorizer.fit_transform(tweets_corpus.content)\n",
    "corpus_data_features_nd = corpus_data_features.toarray() #toarray to see the 2d-array of the sparse matrix that we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5066x144 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16567 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7814785301237362"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(\n",
    "    model,\n",
    "    corpus_data_features_nd[0:len(tweets_corpus)],\n",
    "    y=tweets_corpus.polarity_bin,\n",
    "    scoring='roc_auc',\n",
    "    cv=5\n",
    "    )\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('keikoTest.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajustamos el modelo at corpus de TASS\n",
    "pipeline.fit(tweets_corpus.content, tweets_corpus.polarity_bin)\n",
    "#Usamoslos tweets de keiko para hallar laoplaridad\n",
    "tweets['polarity'] = pipeline.predict(tweets.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>#Hoy #Lambayeque\\r\\n#Precandidatos #PrensaRegi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Hasta pronto #Ica!!! Una gran visita con un gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Hacer acusaciones a la ligera sobre personas i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Quedan casi 4 años de gobierno. Tiempo suficie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RT @RedesKeiko: Mariella Balbi en @Peru21notic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Este gobierno creía que con la plata se compra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Presidente Kuczynski, usted dice que el indult...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Vengo siendo investigada 18 meses por el tema ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Lo digo por enésima vez: ni Fuerza Popular ni ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Resulta que como ahora la Fiscalía \"necesita m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  polarity\n",
       "100  #Hoy #Lambayeque\\r\\n#Precandidatos #PrensaRegi...         1\n",
       "92   Hasta pronto #Ica!!! Una gran visita con un gr...         1\n",
       "103  Hacer acusaciones a la ligera sobre personas i...         0\n",
       "144  Quedan casi 4 años de gobierno. Tiempo suficie...         0\n",
       "29   RT @RedesKeiko: Mariella Balbi en @Peru21notic...         0\n",
       "106  Este gobierno creía que con la plata se compra...         1\n",
       "189  Presidente Kuczynski, usted dice que el indult...         1\n",
       "134  Vengo siendo investigada 18 meses por el tema ...         0\n",
       "153  Lo digo por enésima vez: ni Fuerza Popular ni ...         1\n",
       "133  Resulta que como ahora la Fiscalía \"necesita m...         0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[['content', 'polarity']].sample(10)\n",
    "#1 positivo\n",
    "#0   negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = [ tweet for index, tweet in enumerate(tweets['content']) if tweets['polarity'][index] > 0]\n",
    "neg_tweets = [ tweet for index, tweet in enumerate(tweets['content']) if tweets['polarity'][index] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive tweets: 65.5%\n",
      "Percentage of negative tweets: 34.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(tweets['content'])))\n",
    "print(\"Percentage of negative tweets: {}%\".format(len(neg_tweets)*100/len(tweets['content'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
